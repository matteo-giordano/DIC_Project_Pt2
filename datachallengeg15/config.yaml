seed: 69
model_path: "ppo_maze_model.pth"

PPO:
  state_dim: 5
  action_dim: 8
  hidden_size: 128
  lr_actor: 0.0005
  lr_critic: 0.0005
  gamma: 0.99
  clip_epsilon: 0.2
  k_epochs: 4
  entropy_coef: 0.01
  max_grad_norm: 0.5
  memory_size: 4096
  batch_size: 64
  gae_lambda: 0.95
  recent_history_length: 16

reward:
  name: RewardFunction
  args:
    success_reward: 120.0
    distance_penalty_coef: 0.1
    step_penalty: 0.01
    loop_penalty: 0.5

trainer:
  episodes: 1000
  max_steps: 250
  update_frequency: 5
  early_stop_success_rate: 100.0
  early_stop_patience: 10
  enable_live_tracking: true
  test_episodes: 2

env:
  map_path: "datachallengeg15/warehouse.npy"
  name: MultiTargetEnvironment
  step_size: 0.4
  goals: [[15.8, 37], [17, 18], [5, 6], [2, 35]] # only for MultiTargetEnvironment
  start_pos: [15.8, 37] # Only for Environment

  
