seed: 69
model_path: "ppo_maze_model.pth"

PPO:
  state_dim: 11
  action_dim: 8
  hidden_size: 128
  lr_actor: 0.0001
  lr_critic: 0.0001
  gamma: 0.99
  clip_epsilon: 0.2
  k_epochs: 4
  entropy_coef: 0.01
  max_grad_norm: 0.5
  memory_size: 2048
  batch_size: 64
  gae_lambda: 0.95
  recent_history_length: 16

reward:
  name: TunedRewardFunction
  args:
    success_reward: 100.0
    distance_penalty_coef: 0.1
    step_penalty: 0.001
    loop_penalty: 1

trainer:
  episodes: 1380
  max_steps: 250
  update_frequency: 5
  early_stop_success_rate: 100.0
  early_stop_patience: 2
  enable_live_tracking: true
  test_episodes: 2

env:
  map_path: "datachallengeg15/warehouse.npy"
  name: Environment
  step_size: 0.4
  goals: [[15.8, 37], [17, 18], [5, 6], [2, 35]] # only for MultiTargetEnvironment
  start_pos: [15.8, 37] # Only for Environment

  
